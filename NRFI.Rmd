---
title: "NRFI Prediction Model"
author: "Bryan Jacobs\nAiden Fletcher"
date: "2025-08-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## NRFI Prediction Model

Goal: Predict the probability of the occurrence of a No Run First Inning given any MLB match up.

Data Set: baseballr

Model: XGBoost

Pull data from baseballr using R

potentially cleaning / feature engineering

```{r}
# Load Packages
if (!requireNamespace('pacman', quietly = TRUE)){
  install.packages('pacman')
}
pacman::p_load_current_gh("BillPetti/baseballr")

pacman::p_load(tidyverse,
               reticulate,
               purrr,
               progress)
```

```{r}
# Massive list of game IDs
game_ids <- c(718778, 718779, 718780) 

# Safe wrappers
safe_linescore <- safely(mlb_game_linescore)
safe_gameinfo  <- safely(mlb_game_info)
safe_probables <- safely(mlb_probables)

# Progress bar
pb <- progress_bar$new(total = length(game_ids))

# Fetch inning-by-inning data ################################################################
all_games_data <- map_df(game_ids, ~ { pb$tick(); safe_linescore(.x)$result })

# Filter to inning 1, keep only pre-game predictors + outcome vars for target creation
inning_1_data <- all_games_data %>%
  filter(num == 1) %>%
  select(game_pk, 
         num, 
         home_team_name, 
         away_team_name,
         home_runs,  # will be used for NRFI target
         away_runs,
         home_team_record_wins, 
         away_team_record_wins)

# Fetch weather data #########################################################################
weather_data <- map_df(game_ids, ~ safe_gameinfo(.x)$result)

# Clean and rename to avoid collisions
weather_data_clean <- weather_data %>%
  select(game_pk,
         venue_name,
         temp_game = temperature,
         weather = other_weather)

# Fetch starting pitcher and umpire data #####################################################
pitcher_data <- map_df(game_ids, ~ safe_probables(.x)$result)

# Extract home and away teams from original data
game_teams = all_games_data %>%
  select(game_pk,
         home_team_id,
         away_team_id)

# Join pitcher data with game_teams on game_pk
pitcher_with_side <- pitcher_data %>%
  left_join(game_teams, by = "game_pk") %>%
  mutate(home_away = case_when(
    team_id == home_team_id ~ "home",
    team_id == away_team_id ~ "away",
    TRUE ~ NA_character_
  ))

# Split pitchers to home and away
home_pitchers <- pitcher_with_side %>%
  filter(home_away == "home") %>%
  select(game_pk, home_team_name = team, home_pitcher_name = fullName)

away_pitchers <- pitcher_with_side %>%
  filter(home_away == "away") %>%
  select(game_pk, away_team_name = team, away_pitcher_name = fullName)

# Extract unique rows
home_pitchers_unique <- home_pitchers %>%
  group_by(game_pk, home_team_name) %>%
  slice(1) %>%  # keep only first row per group
  ungroup()

away_pitchers_unique <- away_pitchers %>%
  group_by(game_pk, away_team_name) %>%
  slice(1) %>%
  ungroup()


# Join, create NRFI target, drop leak columns ################################################
inning_1_final <- inning_1_data %>%
  left_join(weather_data_clean, by = "game_pk") %>%
  left_join(home_pitchers_unique, by = c("game_pk", "home_team_name")) %>%
  left_join(away_pitchers_unique, by = c("game_pk", "away_team_name")) %>%
  mutate(NRFI = as.integer(home_runs == 0 & away_runs == 0)) %>%
  select(-home_runs, -away_runs, -game_pk, -num)

# write final data to a csv
write.csv(inning_1_final, "inning_1_data.csv", row.names = FALSE)
```

model creation, training, testing, optimizing in Python

```{python}
# Load packages
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import joblib
```

```{python}
# ===== Step 1: Load your CSV =====
# Replace with your file path
df = pd.read_csv("your_file.csv")

# ===== Step 2: Separate features (X) and target (y) =====
# Replace 'target_column' with your target variable name
X = df.drop(columns=["target_column"])
y = df["target_column"]

# ===== Step 3: Identify categorical & numeric columns =====
categorical_cols = X.select_dtypes(include=["object", "category"]).columns
numeric_cols = X.select_dtypes(exclude=["object", "category"]).columns

# ===== Step 4: Build preprocessing for categorical data =====
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ("num", "passthrough", numeric_cols)
    ]
)

# ===== Step 5: Create the XGBoost model pipeline =====
model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", XGBClassifier(
        objective="binary:logistic",  # Change if multi-class
        eval_metric="logloss", 
        use_label_encoder=False
    ))
])

# ===== Step 6: Train/test split =====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ===== Step 7: Train the model =====
model.fit(X_train, y_train)

# ===== Step 8: Predictions and accuracy =====
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc:.2f}")
```

```{python}
# Save trained model to avoid retraining
joblib.dump(model, "xgb_game_model.pkl")
```

```{python}
# Streamlit Web App

```
